# Spiking Orchestrator (SORCH): SNN駆動型・自律対話エージェント 仕様書

## 1. プロジェクト概要
### 1.1. 目的
単に言葉を生成するチャットボットではなく、人間のような**「会話の間（ま）」「テンポ」「割り込みへの反応」**を備えた、生物的な実在感のあるAIエージェントを構築する。
**最重要目標は、エンドツーエンド（マイク入力〜音声停止）の実測遅延 30-50ms の達成と、誤検知（咳払い等への反応）の排除である。**
また、平均遅延だけでなく**95th/99thパーセンタイル（テールレイテンシ）**を管理し、体験のばらつき（ジッタ）を抑える。

### 1.2. コア・コンセプト：Bi-Level Architecture & Split Core
システムを「道具（Tools）」と「中枢（Core）」に分離し、さらに中枢内部を「反射（固定）」と「記憶（可塑）」に分割することで、堅牢性と柔軟性を両立する。
* **Tools:** HuBERT, RWKV, VOICEVOX (すべて既存モデル・APIを利用)。
* **Core:** 軽量SNN。ここが**SORCH**の本体であり、Python (SpikingJelly) 上で動作する。

## 2. システムアーキテクチャ
| レイヤー | モジュール名 | 役割 (Role) & 実装要件 |
| :--- | :--- | :--- |
| **感覚層** | **Multi-Feature Sensory** | 音声を「物理的特徴（音量・ピッチ・スペクトル）」と「意味（ベクトル）」に分離入力。 |
| **中枢層** | **Orchestrator Core** | **【開発の核心】** 以下の2領域による協調制御。<br>1. **Reflex Circuit** (反射・安全装置)<br>2. **Hippocampal Reservoir** (短期記憶・文脈) |
| **思考層** | **Cognitive Engine** | RWKV (Frozen)。中枢からのリクエスト時のみ推論を行う。 |
| **運動層** | **Motor Cortex** | **Speech Plan駆動のChunk-based TTS**。発話内容（テキスト）と話し方（抑揚/間/強調/付け足し方針）を分離して受け取り、短い単位で生成・差し替え・即時停止できる。実装はSNN単体に拘らず、**SNN（ゲート/トリガ）＋制御ロジック（状態機械）** の複合を基本とする。 |

## 3. モジュール詳細仕様：Orchestrator Core
中枢層は時間解像度 $\Delta t = 1.0 \text{ms}$ (推奨) で動作するSNNシミュレーションである。
クリティカルパスの遅延安定性のため、Pythonのメインループではなく**マルチプロセス（共有メモリ/Lock-free Ring Buffer）**での実装を推奨する。

### 3.1. 領域1: Reflex Circuit (反射回路)
* **役割:** 割り込み検知と即時停止。
* **特性:** 重み固定。誤検知を防ぐための**不感期（Hold-off）**に加え、**環境適応性**と**自己発話抑制**を持つ。
* **入力:** 多次元特徴量アンサンブル (Envelope, ZCR, Spectral Centroid, Voicing Flag)。
* **出力:** `Stop_Signal` (運動層への抑制スパイク)。
* **誤検知対策 (Robustness Logic):**
    1.  **不感期 (Hold-off):** Stop信号発火後、**200-500ms** の不感期を設け、チャタリングを防止する（状況依存で可変とする）。
    2.  **SNR適応閾値 (Adaptive Threshold):** 以下の指数移動平均を用いて動的閾値を設定する。
        $$V_{th}(t) = \mu_{\text{noise}}(t) + \alpha, \quad \mu_{\text{noise}}(t) = (1-\gamma)\mu_{\text{noise}}(t-1) + \gamma E(t)$$
        ※ $\gamma$: 減衰係数（時定数1秒相当）、$E(t)$: 非音声区間のエネルギー
    3.  **ループバック検知 (Loopback/Echo Suppression):** 単なるフラグだけでなく、**AEC (Acoustic Echo Cancellation)** または近接マイクの差分検出を併用し、自身の音声を確実にマスクする。

### 3.2. 領域2: Hippocampal Reservoir (海馬リザーバ)
* **役割:** 短期記憶（会話の勢い、相槌タイミング）。
* **特性:** **STP (Short-Term Plasticity)** 実装。
* **入力:** `Semantic_Vector` (意味) + `Reflex_Output` (反射回路の状態)。
* **Readout:** 高次元状態ベクトルをランダム射影等で圧縮し、**RLS (Recursive Least Squares)** でアクションを決定。
    * **安定化策:** RLSには **忘却係数 $\lambda$ (0.99-0.999)** と **Tikhonov正則化** を導入し、数値的安定性を確保する。

### 3.3. Orchestrator → Motor の出力インターフェース（Speech Plan）
Orchestratorは「何を言うか（文章）」と「どう言うか（抑揚/間/強調/付け足し）」を分離する。

目的:
- 抑揚や言い直し・付け足しまでOrchestratorに学習させると、状態・目的が増えて学習が不安定になりやすい
- そこでOrchestratorは **低次元のメタ情報（Speech Plan）** までを出力し、音声表現の詳細はMotor Cortex側で実装する

Speech Plan（概念）:
- `text`: 発話テキスト
- `intent`: 発話意図（例: ack/ask/explain/confirm/stop などの離散ラベル）
- `delivery`: 話し方の粗い制御（低次元連続）
    - 例: arousal, warmth, urgency, confidence, pause_bias
- `revision_policy`: 付け足し/言い直し/途中割り込み時の方針（離散）
    - 例: append_only / replace_future_chunks / cancel_and_restart

※ 重要: deliveryは「声質パラメータの直接指定」ではなく、Motorが実装するマッピングの入力とする。

## 4. 運動層仕様 (Motor Cortex)
Motor Cortexは「音声として自然に聞こえる」ことの責務を持つ。Orchestratorの学習負荷を増やさないため、
抑揚・ポーズ・付け足し等はMotorの内部実装で吸収する。

実装方針（重要）:
- Motor CortexはOrchestratorと同じくCore側に属するが、**SNNだけで完結させることを必須にしない**。
- 安全性と実装容易性のため、**SNN（割り込み/ゲート/優先度）＋制御ロジック（キュー管理・差し替え・TTSパラメータ適用）** の複合で実装する。

1. **入力: Speech Plan**
    - Orchestratorから `Speech Plan`（テキスト + 話し方メタ情報）を受け取って動作する。
    - Reflexからの `Stop_Signal` は最優先で割り込み処理される。

2. **プロソディ生成（抑揚/間/強調）: テキスト規則 + deliveryの合成**
    - 句読点、疑問文、感嘆、強調語などの **テキスト由来の規則**で基本の抑揚を決める。
    - `delivery`（低次元）で全体傾向を上書きする（例: arousal↑で intonation/volume↑、pause_bias↑でポーズ長↑）。
    - 実装はVOICEVOX等のTTSパラメータ（speed/pitch/intonation/volume、アクセント句のポーズ等）へのマッピングとする。

3. **ストリーミング再生 (Chunking):**
    - 音声を一括生成せず、短いチャンク（200-500ms単位）で順次生成・再生する。
    - これにより「途中で止める」「次のチャンクを差し替える」が可能になる。

4. **付け足し/言い直し（Revision）: チャンク差し替え戦略**
    - Orchestratorの `revision_policy` に従い、
      - 付け足し: 後続チャンクに追記
      - 言い直し: 未来のチャンクを差し替え（必要なら短いポーズ/クロスフェード）
    - 原則として「すでに再生済みの音声」を巻き戻さず、キューの後半で自然に整合を取る。

5. **即時キャンセル (Hard Stop):**
    - `Stop_Signal` を受信した瞬間、**「再生キューのクリア」**と**「オーディオデバイスのリセット」**を実行する（APIレベルでのプリエンプティブ性の確認必須）。

---

## 5. 開発ロードマップ (Detailed)

「まず実測し、現実を知る」ことから開始し、最終的に自律的な対話を実現する。

### Phase 1: ベンチマークと反射回路の実装
**目的:** エンドツーエンド遅延の実測と、誤検知のない割り込み機能の実現。

* **Step 1.0: 遅延計測ベンチマーク (Must Do First)**
    * 単純な「マイク入力 → 閾値判定 → スピーカー停止命令」のループスクリプトを作成。
    * **レイテンシ予算の分解と計測:**
        1. マイク取り込み〜フレーム確定: 目標 1-3ms
        2. 前処理 (Feature Extraction): 目標 1-5ms
        3. 判定 (Inference): 目標 < 1ms
        4. 出力停止実行 (OS/Driver): 目標 1-5ms
    * **判定基準:** Median Latency < 50ms かつ **p99 < 100ms**。未達の場合は C++/Rust ネイティブ実装へ移行。
* **Step 1.1: Reflex Circuit のプロトタイプ（暫定DSP → 将来SNNへ）**
    * SNNの最適化には時間がかかるため、まずは**古典的信号処理（VAD + 適応閾値）**で反射回路を実装し、遅延とFP/FNのベースラインを確定させる。
    * 並行してSNNモデルの研究開発を進め、段階的に **SNN版へ置換可能** な構造にしておく（固定重み + 安全ロジック等）。
* **Step 1.2: 停止制御（Hard Stop：TTS等を含む停止）**
    * VOICEVOX CORE等のAPIを確認し、バッファ破棄を含む安全な停止処理を実装する。
    * 具体的には **再生キューのクリア** と **オーディオデバイス停止/リセット** を扱い、E2EでStopのテールレイテンシ（p95/p99）を計測する。

### Phase 2: 記憶回路と文脈制御
**目的:** 会話の「間」と「文脈」を液状記憶（Reservoir）として実装する。ここでは**STP（短期可塑性）の挙動**を作り込むことが主眼となる。

* **Step 2.1: カスタムSTPニューロンの実装**
    * **内容:** SpikingJellyの `BaseNode` を継承し、Tsodyks-Markramモデルの微分方程式（$du/dt, dx/dt$）を内部状態として持つ `STP_LIFNode` クラスを作成する。
* **Step 2.2: Memory Capacity (MC) の最大化探索**
    * **内容:** 500〜1000個のニューロンを持つリザーバ層を構築。ランダムな入力パターンを流し込み、過去の入力が現在のリザーバ状態からどれだけ復元できるかを計測する。
    * **探索パラメータ:** $\tau_F$ (100-800ms), $\tau_D$ (500-3000ms), $W_{scale}$ (0.8-1.1)。
* **Step 2.3: Readout層の実装と次元削減**
    * **設計:** Readoutが参照する状態ベクトルは、膜電位 `v` だけでなく **spike** や **STP内部状態（u/r/eff）** を含める候補を比較し、MCなどの指標で採否を決める。
    * **補足:** 状態を増やすほど性能（MC）は伸びやすい一方、次元が増えて計算が重くなる。必要なら Random Projection で圧縮し、性能と軽さのトレードオフで決める。
    * **対策:** **ランダム射影 (Random Projection)** を用い、状態ベクトルを低次元に圧縮してからReadout層に入力する。計算コストの高いPCAは避ける。
	* **暫定推奨（実装運用）:** 軽さ優先の現実案として `state_mode=v+spike` + `proj_out_dim=200` を基準に進める。
* **Step 2.4: オンライン学習 (RLS) の導入**
    * **内容:** ユーザーごとの会話テンポに適応するため、Readout層の重みを RLS (with forgetting factor) でリアルタイム更新する。更新間隔は 10-50ms を目安とする。
    * **補足（運用）:** 更新間隔を落とす場合でも「サンプルを捨てる間引き」ではなく、観測をまとめる **ブロック更新** を前提にして破綻しにくくする。
    * **暫定推奨（ベンチ運用）:** まずは `lam=1.0` を基本とし、忘却を入れる場合は `lam=0.999` から慎重に下げる（過度に下げると不安定化しやすい）。

### Phase 3: システム統合と非同期制御
**目的:** 耳・脳・口を並列動作させ、実用的な応答速度を持つ対話システムとして組み上げる。

* **Step 3.1: 運動野の表現力（Speech Plan / プロソディ / Revision）**
    * **内容:** Orchestratorの学習を複雑にしすぎないため、発話の詳細表現はMotor側に寄せ、Orchestrator→Motorは **Speech Plan（テキスト + 低次元の話し方メタ情報）** を契約として渡す。
    * **要点:** チャンク生成/再生、差し替え（Revision）、およびHard Stop（キュークリア + デバイス停止）を一貫したI/Fで扱えること。

* **Step 3.2: マルチプロセス・アーキテクチャの構築**
    * **内容:** PythonのGIL回避のため、`multiprocessing` を用いてプロセス分離を行う。
        1.  **Audio Process:** マイク/スピーカー (High Priority / RT Kernel推奨)。
        2.  **Core Process:** SNN/Reflex Logic（Reflex Circuit / Hippocampal Reservoir を含む）。
        3.  **Cognitive Process:** RWKV/HuBERT。
        4.  **Motor Process:** TTSチャンク生成/再生キュー/Revision/Hard Stop（Stopは最優先で割り込み）。
* **Step 3.3: Lock-free Queue による通信**
    * **内容:** プロセス間通信には遅延の少ない **Lock-free Ring Buffer (SPSC)** 等を採用する。
* **Step 3.4: 優先度付き制御ロジック**
    * **内容:** `Motor Cortex` において、Stop信号を物理レベルで優先するロジックを実装。
    * **要点（実装指針）:**
        * 通常経路（リング等）の混雑に依存しない **最優先経路（共有フラグ等）** を用意する。
        * 最優先経路は、原則として **Core Process が publish** し、**Motor Process が監視**して即時に停止へ遷移する（通常経路のメッセージ受信を待たない）。
        * 出力は「無音化」だけでなく、可能なら **ストリーム停止（callback完了等）→ close →（任意で）再open** まで行い、物理レベルの停止を担保する。
    * **計測:** publish→検知（即時無音化）と、publish→物理停止（ストリームinactive/close）を分けて JSONL に記録し、p95/p99を確認する。

### Phase 4: UX向上と常駐化
* **Step 4.1: 脳波モニター (Brain Visualizer) の作成**
* **Step 4.2: 会話パラメータの感覚的チューニング**

## 6. 評価指標 (KPI)

開発の進捗は以下の数値で管理する。**特にテールレイテンシとジッタを重視する。**

1.  **Latency (遅延):**
    * Stop Latency: **Median < 50ms**, **p95 < 80ms**, **p99 < 100ms**
    * Jitter (標準偏差): 小さいほど良い
2.  **Safety (誤検知):**
    * False Positive Rate: **< 0.5回 / 分** (実環境での長期テスト必須)
    * False Negative Rate: **< 5%**
3.  **Experience (体験):**
    * Sputtering: **0回** (不感期による制御)

## 7. 開発環境・技術スタック

* **言語:** Python 3.12 (ホットパスの Cython/Numba 化、または C++/Rust への移行を視野)
* **SNNライブラリ:** SpikingJelly
* **Audio I/O:**
    * Linux: **JACK** または ALSA (バッファサイズ 64-128 samples推奨)
    * Windows: ASIO / WASAPI-Exclusive
    * Library: `PyAudio` (要低遅延設定) または `python-sounddevice`
* **Signal Processing:** `librosa`, `numpy`
* **Compute:** NVIDIA GPU (CUDA) ※ただしリアルタイムループはCPU推奨
* **OS:** Linux (Realtime Kernel / PREEMPT_RT 推奨)
* **Deployment:** Docker (Host Audio Passthrough必須)

---

## 8. リポジトリ構成と成果物の置き場所（運用）

初めて触る人が迷子になりやすいため、資料と成果物の置き場所を固定する。

### 8.1. ドキュメント（docs/）
- `docs/ガイド.md`: 入口（この1本だけ追えばOK）
- `docs/spec/`: 仕様書（本書）
- `docs/howto/`: 手順書（遅延計測、MC探索など）
- `docs/project/`: チェックリスト等の進捗管理

### 8.2. 生成物（outputs/）
実験・計測の生成物は `outputs/` 配下に保存する。
- `outputs/phase1/latency/`: Phase 1 遅延計測の JSONL ログ（stopイベント単位）
- `outputs/phase1/report/`: Phase 1 の測定結果まとめ（Markdown）
- `outputs/phase2/mc/runs/`: Phase 2（MC探索）の生CSV
- `outputs/phase2/mc/reports/`: Phase 2（MC探索）の集計レポート（Markdown）

※ 生成物はサイズが増えるため、Git管理するかは運用ポリシーに従う。
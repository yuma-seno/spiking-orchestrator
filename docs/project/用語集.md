# 用語集（非エンジニア向け）

このプロジェクトは「音声の停止を素早く・安全に行う」ことから始めています。
ここでは、資料中に出てくる用語をざっくり理解できるように説明します。

<a id="glossary-stop"></a>
## Stop（停止）
話している途中でも、ユーザーが割り込んだら **すぐに音声を止める**こと。
体験としては「喋り続けない」「反応が遅れない」に直結します。

<a id="glossary-latency"></a>
## レイテンシ（遅延）
何かが起きてから反応するまでの時間。
ここでは主に「停止の判断をして止めるまで」の時間を測ります。

<a id="glossary-percentile"></a>
## p95 / p99（パーセンタイル）
ざっくり言うと「ほとんどのケースは速いけど、たまに遅い」が問題になります。
- **p95**: 100回中95回はこれ以下の遅延
- **p99**: 100回中99回はこれ以下の遅延

平均よりも **p99のような“たまに遅い”** が体験を壊しやすいので仕様書で重視しています。

<a id="glossary-reflex"></a>
## Reflex（反射回路）
「細かく考える前に、安全のために即反応する」仕組み。
このプロジェクトではまず「割り込みを検知して止める」役割です。

<a id="glossary-dsp-first"></a>
## DSP-first（古典的信号処理から始める）
最初から難しい学習モデルにせず、
まずは音量などの単純な特徴量で動く仕組みを作ってベースライン（基準値）を取る方針です。

<a id="glossary-snn"></a>
## SNN（スパイキングニューラルネットワーク）
脳のニューロンのように、連続値ではなく“スパイク（発火）”で情報をやり取りするモデル。
このプロジェクトでは SpikingJelly で試作しています。

<a id="glossary-stp"></a>
## STP（短期可塑性）
短時間だけ反応が強くなったり弱くなったりする性質。
会話テンポの“勢い”を持たせるのに使えると期待しています。

<a id="glossary-mc"></a>
## Memory Capacity（MC）
短期記憶の強さを測る指標。
「少し前の入力を、今の状態からどれだけ復元できるか」を数値化します。

---

<a id="glossary-readout"></a>
## Readout（読み出し層）
リザーバ（脳っぽい回路）の「いまの状態」を見て、
最終的な出力（たとえば“止める/止めない”や“次の行動”）を決める **最後の薄い層**です。

ポイント:
- リザーバ本体は複雑でも、Readoutは **線形（足し算）** で済むことが多い
- 「何を見せるか（状態ベクトル設計）」が性能に効きやすい

<a id="glossary-state-vector"></a>
## 状態ベクトル（state vector）
ある時刻 $t$ におけるリザーバの内部状態を、数の並び（ベクトル）として取り出したもの。
例: 膜電位 `v`、スパイク `spike`、STP内部状態 `u/r/eff` など。

<a id="glossary-state-mode"></a>
## state_mode（状態ベクトルの選び方）
Readoutに渡す「状態ベクトル」を、どの信号の組み合わせにするかを表す指定です。
このリポジトリでは `v+spike+u+r+eff` のように `+` で連結して表します。

<a id="glossary-spike-rate"></a>
## spike_rate（発火率）
「1ステップ×1ニューロンあたり、平均でどれくらいスパイクが出ているか」の目安。

直感:
- 0に近い: ほぼ発火していない（動いていない）可能性
- 大きすぎる: 常に発火して飽和している可能性（これも記憶が出にくいことがある）

<a id="glossary-washout"></a>
## washout（ウォッシュアウト）
実験の最初は、状態が落ち着いていない“ならし運転”の時間があるため、
その区間を **評価から捨てる** ための設定です。

<a id="glossary-max-delay"></a>
## max_delay（最大遅れ）
MCで「どれくらい昔の入力まで復元できるか」を測るときの範囲。
`max_delay=120` なら「1〜120ステップ前」までを評価します。

<a id="glossary-ridge"></a>
## ridge（リッジ正則化）
Readoutを学習するときの“暴れ防止”の設定。
大きくすると学習が安定しやすい一方、当てはまりが弱くなることがあります。

<a id="glossary-random-projection"></a>
## Random Projection（ランダム射影）
高次元の状態ベクトルを、ランダムな行列で **低次元に圧縮** する方法。

狙い:
- 次元が大きいと計算が重い → 圧縮して軽くする
- できるだけ情報を壊さずに圧縮したい

<a id="glossary-rls"></a>
## RLS（Recursive Least Squares）
Readoutの重みを、データが流れてくるたびに **オンラインで更新** する学習法。
「ユーザーの会話テンポが変わる」などの変化に追従する用途を想定しています。

---

<a id="glossary-speech-plan"></a>
## Speech Plan（発話計画）
「何を言うか（文章）」と「どう言うか（話し方）」を分けて、Motor Cortexに渡すためのデータ。

狙い:
- 抑揚や言い直しまでOrchestratorに学習させると複雑になりすぎる
- そこでOrchestratorは **低次元のメタ情報**だけを渡し、音声表現の詳細はMotorが担当する

例（概念）:
- `text`: 発話テキスト
- `intent`: 発話意図（相槌/質問/説明など）
- `delivery`: 話し方の粗い指定（元気さ、急ぎ、間の長さなど）
- `revision_policy`: 付け足し/言い直しの方針

<a id="glossary-prosody"></a>
## プロソディ（prosody, 抑揚・間・アクセント）
同じ文章でも「疑問っぽく聞こえる」「強調して聞こえる」など、話し方のニュアンスを決める要素。
句読点や文末（？/！）などのテキスト情報と、話し方メタ情報（Speech Plan）から生成します。

<a id="glossary-revision"></a>
## Revision（付け足し・言い直し）
発話の途中で「補足したい」「言い直したい」が起きたときに、音声再生を自然に更新すること。
このプロジェクトでは、短いチャンク単位で再生し、未来のチャンクを差し替える方式を想定します。

<a id="glossary-forgetting-factor"></a>
## 忘却係数（forgetting factor, lambda）
RLSで「昔のデータをどれくらい忘れるか」を決める値。

直感:
- 1.0に近い: ほぼ忘れない（安定だが追従が遅い）
- 小さめ: 早く忘れる（追従は速いが不安定になりやすい）

<a id="glossary-tikhonov"></a>
## Tikhonov正則化（チホノフ）
学習が不安定にならないように、重みが大きくなりすぎるのを抑える工夫。
このリポジトリではRLSやridgeの文脈で出てきます。

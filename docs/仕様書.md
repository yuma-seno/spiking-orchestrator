# Spiking Orchestrator (SORCH): SNN駆動型・自律対話エージェント 仕様書

## 1. プロジェクト概要
### 1.1. 目的
単に言葉を生成するチャットボットではなく、人間のような**「会話の間（ま）」「テンポ」「割り込みへの反応」**を備えた、生物的な実在感のあるAIエージェントを構築する。
**最重要目標は、エンドツーエンド（マイク入力〜音声停止）の実測遅延 30-50ms の達成と、誤検知（咳払い等への反応）の排除である。**

### 1.2. コア・コンセプト：Bi-Level Architecture & Split Core
システムを「道具（Tools）」と「中枢（Core）」に分離し、さらに中枢内部を「反射（固定）」と「記憶（可塑）」に分割することで、堅牢性と柔軟性を両立する。
* **Tools:** HuBERT, RWKV, VOICEVOX (すべて既存モデル・APIを利用)。
* **Core:** 軽量SNN。ここが**SORCH**の本体であり、Python (SpikingJelly) 上で動作する。

## 2. システムアーキテクチャ
| レイヤー | モジュール名 | 役割 (Role) & 実装要件 |
| :--- | :--- | :--- |
| **感覚層** | **Multi-Feature Sensory** | 音声を「物理的特徴（音量・ピッチ・スペクトル）」と「意味（ベクトル）」に分離入力。 |
| **中枢層** | **Orchestrator Core** | **【開発の核心】** 以下の2領域による協調制御。<br>1. **Reflex Circuit** (反射・安全装置)<br>2. **Hippocampal Reservoir** (短期記憶・文脈) |
| **思考層** | **Cognitive Engine** | RWKV (Frozen)。中枢からのリクエスト時のみ推論を行う。 |
| **運動層** | **Motor Cortex** | **Chunk-based TTS**。短い単位での生成と、バッファ即時破棄機能を持つ。 |

## 3. モジュール詳細仕様：Orchestrator Core
中枢層は時間解像度 $\Delta t = 1.0 \text{ms}$ (推奨) で動作するSNNシミュレーションである。

### 3.1. 領域1: Reflex Circuit (反射回路)
* **役割:** 割り込み検知と即時停止。
* **特性:** 重み固定。誤検知を防ぐための**不感期（Hold-off / Refractory）**を持つ。
* **入力:** 多次元特徴量アンサンブル (Envelope, ZCR, Spectral Centroid, Voicing Flag)。
* **出力:** `Stop_Signal` (運動層への抑制スパイク)。
* **誤検知対策:** Stop信号発火後、**200-500ms** の不感期を設け、チャタリングを防止する。

### 3.2. 領域2: Hippocampal Reservoir (海馬リザーバ)
* **役割:** 短期記憶（会話の勢い、相槌タイミング）。
* **特性:** **STP (Short-Term Plasticity)** 実装。
* **入力:** `Semantic_Vector` (意味) + `Reflex_Output` (反射回路の状態)。
* **Readout:** 高次元状態ベクトルを PCA/ランダム射影で圧縮し、Ridge回帰またはRLSでアクションを決定。

## 4. 運動層仕様 (Motor Cortex)
1.  **ストリーミング再生 (Chunking):** 音声を一括生成せず、短いチャンク（200-500ms単位）で順次生成・再生する。
2.  **即時キャンセル (Hard Stop):** Reflexからの `Stop_Signal` を受信した瞬間、**「再生キューのクリア」**と**「オーディオデバイスのリセット」**を実行する。

---

## 5. 開発ロードマップ (Detailed)

「まず実測し、現実を知る」ことから開始し、最終的に自律的な対話を実現する。

### Phase 1: ベンチマークと反射回路の実装
**目的:** エンドツーエンド遅延の実測と、誤検知のない割り込み機能の実現。

* **Step 1.0: 遅延計測ベンチマーク (Must Do First)**
    * 単純な「マイク入力 → 閾値判定 → スピーカー停止命令」のループスクリプトを作成。
    * 4点のタイムスタンプ (Input, Detect, Call, Effective) を記録。
    * **判定基準:** Median Latency < 50ms ならPython続行。超える場合は `cython` / `numba` 化、またはホストOS直結を検討。
* **Step 1.1: Reflex Circuit の学習**
    * 多次元特徴量（Envelope, ZCR, Spectral）を入力とするSNNを学習。
    * FP (誤検知) 率 < 0.5回/分 を目標とする。
* **Step 1.2: 停止制御の実装**
    * VOICEVOX CORE等のAPIを確認し、バッファ破棄を含む安全な停止処理を実装する。

### Phase 2: 記憶回路と文脈制御
**目的:** 会話の「間」と「文脈」を液状記憶（Reservoir）として実装する。ここでは**STP（短期可塑性）の挙動**を作り込むことが主眼となる。

* **Step 2.1: カスタムSTPニューロンの実装**
    * **内容:** SpikingJellyの `BaseNode` を継承し、Tsodyks-Markramモデルの微分方程式（$du/dt, dx/dt$）を内部状態として持つ `STP_LIFNode` クラスを作成する。
    * **検証:** 連続パルスを入力した際、EPSP（シナプス後電位）が徐々に増幅（Facilitation）または減衰（Depression）することを波形プロットで確認する。
* **Step 2.2: Memory Capacity (MC) の最大化探索**
    * **内容:** 500〜1000個のニューロンを持つリザーバ層を構築。ランダムな入力パターンを流し込み、過去の入力が現在のリザーバ状態からどれだけ復元できるかを計測する。
    * **探索パラメータ:**
        * $\tau_F$ (促通): 100ms 〜 800ms
        * $\tau_D$ (抑圧): 500ms 〜 3000ms
        * $W_{scale}$ (スペクトル半径): 0.8 〜 1.1
* **Step 2.3: Readout層の実装と次元削減**
    * **課題:** リザーバの状態ベクトル（例: 1000次元）をそのまま学習させると不安定になりやすい。
    * **対策:** **ランダム射影 (Random Projection)** または **PCA** を用い、状態ベクトルを低次元（例: 50次元）に圧縮してからReadout層（線形回帰）に入力するパイプラインを構築する。
* **Step 2.4: オンライン学習 (RLS) の導入**
    * **内容:** ユーザーごとの会話テンポに適応するため、Readout層の重みを固定せず、**RLS (Recursive Least Squares)** アルゴリズムを用いて会話中にリアルタイム更新できる仕組みを実装する。

### Phase 3: システム統合と非同期制御
**目的:** 耳・脳・口を並列動作させ、実用的な応答速度を持つ対話システムとして組み上げる。

* **Step 3.1: マルチスレッド・アーキテクチャの構築**
    * **内容:** Pythonの `threading` または `multiprocessing` を用い、以下の3スレッドを独立させる。
        1.  **Audio Thread:** マイク入力(PyAudio) と スピーカー出力(TTS Stream) を担当。最優先度。
        2.  **SNN Loop Thread:** 1ms単位で `Orchestrator Core` を更新し続ける。
        3.  **Cognitive Thread:** 重い処理（RWKV, HuBERT）を担当。SNNからのRequest待ちで動く。
* **Step 3.2: Lock-free Queue による通信**
    * **内容:** スレッド間のデータ受け渡しに `queue.SimpleQueue` (またはより高速なリングバッファ実装) を使用し、排他制御（Lock）による遅延発生を防ぐ。
* **Step 3.3: 優先度付き制御ロジック (Barge-in Logic)**
    * **内容:** `Motor Cortex` において、RWKVからの「発話データ」とReflexからの「停止信号」が衝突した場合、**物理レベルで停止信号を優先**し、発話キューを強制クリアするロジックを実装する。
* **Step 3.4: 状態遷移マシン (FSM) の定義**
    * **内容:** システム全体の状態を `Listening`, `Thinking`, `Speaking`, `Interrupted` の4状態に定義し、SNNの出力によって遷移させる。特に `Speaking` から `Interrupted` への遷移速度（レイテンシ）を徹底的にチューニングする。

### Phase 4: UX向上と常駐化
**目的:** 「実験コード」から「デスクトップ上のパートナー」へと昇華させる。

* **Step 4.1: 脳波モニター (Brain Visualizer) の作成**
    * **内容:** `matplotlib` (高速描画モード) または `PyQtGraph` を使用し、リザーバ層の発火ラスタープロット（脳の活性状況）と、現在のモード（聞いてる/喋ってる）をリアルタイム表示するGUIウィジェットを作成する。デバッグだけでなく、ユーザーへの「生きてる感」の演出としても機能させる。
* **Step 4.2: 会話パラメータの感覚的チューニング**
    * **内容:** 実際に数時間の雑談を行い、以下のパラメータを「心地よい」値に調整する。
        * **割り込み感度:** 咳払いで止まらないか、でも「えっ」と言ったら止まるか。
        * **相槌の頻度:** 喋りづらくない程度に「うん」と言ってくれるか。


## 6. 開発環境・技術スタック

* **言語:** Python
* **SNNライブラリ:** SpikingJelly (PyTorchベースで扱いやすい)
    * Reflex層: `neuron.LIFNode`
    * Reservoir層: Custom Node implementing STP equations
* **ASR/Audio:** HuBERT / PyAudio (または低遅延が必要ならJACK/ASIO対応ライブラリ)
* **LLM:** RWKV-v6 (RWKV-Runner等を利用しAPI化しておくと連携しやすい)
* **TTS:** VOICEVOX CORE (Linux対応、低遅延、キャンセルAPI利用)
* **環境:** Dockerコンテナ (python:3.12-slimベース) ※既にコンテナ内のため、Copilot利用時にはコンテナ作成を行わずローカルでのコマンド実行するよう指示すること。
